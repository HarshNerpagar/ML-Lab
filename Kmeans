import numpy as np
import matplotlib.pyplot as plt

# Function to initialize centroids randomly
def initialize_centroids(X, k):
    indices = np.random.choice(X.shape[0], k, replace=False)
    return X[indices]

# Function to compute Euclidean distances
def compute_distances(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return distances

# Function to assign each point to the closest centroid
def assign_clusters(X, centroids):
    distances = compute_distances(X, centroids)
    return np.argmin(distances, axis=1)

# Function to update centroids
def update_centroids(X, labels, k):
    centroids = np.zeros((k, X.shape[1]))
    for i in range(k):
        points = X[labels == i]
        if len(points) > 0:
            centroids[i] = np.mean(points, axis=0)
        else:
            centroids[i] = X[np.random.choice(X.shape[0])]
    return centroids

# K-Means algorithm implementation
def kmeans(X, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, k)
    for i in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if np.all(np.abs(new_centroids - centroids) < tol):
            print(f"Converged after {i+1} iterations.")
            break
        centroids = new_centroids
    return labels, centroids

# Generate synthetic data
np.random.seed(42)
X1 = np.random.randn(100, 2) + [5, 5]
X2 = np.random.randn(100, 2) + [-5, -5]
X3 = np.random.randn(100, 2) + [5, -5]
X = np.vstack((X1, X2, X3))

# Apply K-Means Clustering
k = 3
labels, centroids = kmeans(X, k)

# Visualize the Clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o', alpha=0.6, edgecolors='k')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("K-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()
